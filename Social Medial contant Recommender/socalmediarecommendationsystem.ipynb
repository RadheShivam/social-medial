{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83601150-cadd-4ef0-ad26-bcd6dd4d987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\shiva\\appdata\\roaming\\python\\python310\\site-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Still useful for small datasets or testing\n",
    "import pickle\n",
    " # FAISS for efficient similarity search\n",
    "get_ipython().system('pip install scikit-learn')\n",
    "get_ipython().system('pip install faiss-cpu') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338d62fc-d152-4005-b74e-f16bbe6635b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "content_data = pd.read_csv('SocialMediaUsersDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f01f36-b23d-41f7-9085-045272f14be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UserID                Name  Gender         DOB  \\\n",
      "0           1       Jesse Lawhorn  Female  1958-10-15   \n",
      "1           2         Stacy Payne  Female  2004-07-21   \n",
      "2           3  Katrina Nicewander  Female  2000-02-07   \n",
      "3           4      Eric Yarbrough    Male  1985-04-14   \n",
      "4           5       Daniel Adkins  Female  1955-09-18   \n",
      "...       ...                 ...     ...         ...   \n",
      "99995   99996      Lionel Denault  Female  1983-07-31   \n",
      "99996   99997     Margie Mieszala  Female  1954-05-20   \n",
      "99997   99998       Joan Mercedes    Male  1975-06-06   \n",
      "99998   99999        Marvin Massa  Female  1959-11-16   \n",
      "99999  100000          Josh Young  Female  1988-07-29   \n",
      "\n",
      "                                               Interests  \\\n",
      "0                'Movies', 'Fashion', 'Fashion', 'Books'   \n",
      "1      'Gaming', 'Finance and investments', 'Outdoor ...   \n",
      "2        'DIY and crafts', 'Music', 'Science', 'Fashion'   \n",
      "3           'Outdoor activities', 'Cars and automobiles'   \n",
      "4                                  'Politics', 'History'   \n",
      "...                                                  ...   \n",
      "99995                                   'DIY and crafts'   \n",
      "99996  'Cars and automobiles', 'Cooking', 'Outdoor ac...   \n",
      "99997         'Business and entrepreneurship', 'Cooking'   \n",
      "99998  'Gaming', 'Business and entrepreneurship', 'Fa...   \n",
      "99999                                 'Politics', 'Pets'   \n",
      "\n",
      "                       City    Country  \n",
      "0                   Sibolga  Indonesia  \n",
      "1                  Al Abyār      Libya  \n",
      "2               Wādī as Sīr     Jordan  \n",
      "3                    Matera      Italy  \n",
      "4                   Biruaca  Venezuela  \n",
      "...                     ...        ...  \n",
      "99995                 Dawan      China  \n",
      "99996  Arcos de la Frontera      Spain  \n",
      "99997             Los Andes      Chile  \n",
      "99998               Varjota     Brazil  \n",
      "99999                 Brzeg     Poland  \n",
      "\n",
      "[100000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(content_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcae40a-6cac-465f-8f27-0f8c107cb7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['UserID', 'Name', 'Gender', 'DOB', 'Interests', 'City', 'Country'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the actual column names to ensure they match\n",
    "print(content_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "354f7400-9a7f-431e-ab5b-baa4a5f9f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns: 'Gender', 'DOB', 'Interests', 'City', 'Country'\n",
    "content_data = content_data[['Gender', 'DOB', 'Interests', 'City', 'Country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e9d68f-9235-4c4b-9673-272b3ccb6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing interests\n",
    "content_data.dropna(subset=['Interests'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef2ec2f-ccfc-4732-9a21-cb9d74243149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender         DOB         City    Country  \\\n",
      "0  Female  1958-10-15      Sibolga  Indonesia   \n",
      "1  Female  2004-07-21     Al Abyār      Libya   \n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan   \n",
      "3    Male  1985-04-14       Matera      Italy   \n",
      "4  Female  1955-09-18      Biruaca  Venezuela   \n",
      "\n",
      "                                                tags  \n",
      "0                       movies fashion fashion books  \n",
      "1  gaming finance and investments outdoor activit...  \n",
      "2               diy and crafts music science fashion  \n",
      "3            outdoor activities cars and automobiles  \n",
      "4                                   politics history  \n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already loaded and preprocessed the content_data DataFrame\n",
    "\n",
    "# Preprocess the 'Interests' column\n",
    "def preprocess_interests(interests):\n",
    "    # Remove single quotes and split by comma\n",
    "    return interests.replace(\"'\", \"\").split(', ') if isinstance(interests, str) else []\n",
    "\n",
    "# Apply preprocessing to the 'Interests' column\n",
    "content_data['Interests'] = content_data['Interests'].apply(preprocess_interests)\n",
    "\n",
    "# Create a 'tags' column by joining interests into a single string\n",
    "content_data['tags'] = content_data['Interests'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Convert the tags to lowercase for uniformity\n",
    "content_data['tags'] = content_data['tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# Create a new DataFrame with relevant columns for recommendation\n",
    "new_df = content_data[['Gender', 'DOB', 'City', 'Country', 'tags']]\n",
    "\n",
    "# Preview the new DataFrame\n",
    "print(new_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e030c6-8d11-476d-82d5-c77c42f13eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the social media dataset: (100000, 6)\n",
      "   Gender         DOB                                          Interests  \\\n",
      "0  Female  1958-10-15                  [Movies, Fashion, Fashion, Books]   \n",
      "1  Female  2004-07-21  [Gaming, Finance and investments, Outdoor acti...   \n",
      "2  Female  2000-02-07          [DIY and crafts, Music, Science, Fashion]   \n",
      "3    Male  1985-04-14         [Outdoor activities, Cars and automobiles]   \n",
      "4  Female  1955-09-18                                [Politics, History]   \n",
      "\n",
      "          City    Country                                               tags  \n",
      "0      Sibolga  Indonesia                       movies fashion fashion books  \n",
      "1     Al Abyār      Libya  gaming finance and investments outdoor activit...  \n",
      "2  Wādī as Sīr     Jordan               diy and crafts music science fashion  \n",
      "3       Matera      Italy            outdoor activities cars and automobiles  \n",
      "4      Biruaca  Venezuela                                   politics history  \n"
     ]
    }
   ],
   "source": [
    "# Assuming 'content_data' is your DataFrame for the social media users\n",
    "# View the shape of the social media dataset\n",
    "print(\"Shape of the social media dataset:\", content_data.shape)\n",
    "\n",
    "# View the first few rows of the social media dataset\n",
    "print(content_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187028bf-9eff-4cf3-99bd-d9c01e110fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender\n",
      "Male      50069\n",
      "Female    49931\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   Gender     100000 non-null  object\n",
      " 1   DOB        100000 non-null  object\n",
      " 2   Interests  100000 non-null  object\n",
      " 3   City       100000 non-null  object\n",
      " 4   Country    100000 non-null  object\n",
      " 5   tags       100000 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 4.6+ MB\n",
      "   Gender         DOB         City    Country  \\\n",
      "0  Female  1958-10-15      Sibolga  Indonesia   \n",
      "1  Female  2004-07-21     Al Abyār      Libya   \n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan   \n",
      "3    Male  1985-04-14       Matera      Italy   \n",
      "4  Female  1955-09-18      Biruaca  Venezuela   \n",
      "\n",
      "                                           Interests  \n",
      "0                  [Movies, Fashion, Fashion, Books]  \n",
      "1  [Gaming, Finance and investments, Outdoor acti...  \n",
      "2          [DIY and crafts, Music, Science, Fashion]  \n",
      "3         [Outdoor activities, Cars and automobiles]  \n",
      "4                                [Politics, History]  \n",
      "Gender       0\n",
      "DOB          0\n",
      "Interests    0\n",
      "City         0\n",
      "Country      0\n",
      "tags         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Value counts for the 'Gender' column (or any relevant column you want to analyze)\n",
    "print(content_data['Gender'].value_counts())\n",
    "\n",
    "# Get information about the dataset\n",
    "content_data.info()\n",
    "\n",
    "# Select specific columns from the content_data dataset\n",
    "print(content_data[['Gender', 'DOB', 'City', 'Country', 'Interests']].head())\n",
    "\n",
    "# Check for missing values\n",
    "print(content_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b6fc40-6716-4975-96bb-dcda35b0325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "content_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae24ce3-5241-4661-b346-05b489b7cb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Movies', 'Fashion', 'Fashion', 'Books']\n"
     ]
    }
   ],
   "source": [
    "# View the 'Interests' column of the first row\n",
    "print(content_data.iloc[0]['Interests'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9579bb-9b44-4010-97c4-c1c63bd9850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # Make sure to import ast at the beginning of your script\n",
    "\n",
    "# Define the function to convert the 'Interests' or other relevant columns\n",
    "def convert(obj):\n",
    "    L = []\n",
    "    # Safely evaluate the string representation of the list\n",
    "    try:\n",
    "        # Convert the string to a list\n",
    "        parsed_list = ast.literal_eval(obj)  # Safely parses the string to list/dict\n",
    "        # Check if parsed_list is indeed a list\n",
    "        if isinstance(parsed_list, list):\n",
    "            for item in parsed_list:  # Iterate through the list\n",
    "                L.append(item.strip())  # Strip any leading/trailing spaces and append\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []  # Return an empty list if there is any issue\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a576f856-5ea4-4458-80f6-83e0a82a01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Interests\n",
      "0        []\n",
      "1        []\n",
      "2        []\n",
      "3        []\n",
      "4        []\n"
     ]
    }
   ],
   "source": [
    "# Apply the convert function to the 'Interests' column\n",
    "content_data['Interests'] = content_data['Interests'].apply(convert)\n",
    "\n",
    "# If there's another relevant column to convert (e.g., 'keywords'), do the same\n",
    "# content_data['keywords'] = content_data['keywords'].apply(convert)\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(content_data[['Interests']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3f3f643-01c3-4ebe-9fb2-a89f380cb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender         DOB         City    Country Interests\n",
      "0  Female  1958-10-15      Sibolga  Indonesia        []\n",
      "1  Female  2004-07-21     Al Abyār      Libya        []\n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan        []\n",
      "3    Male  1985-04-14       Matera      Italy        []\n",
      "4  Female  1955-09-18      Biruaca  Venezuela        []\n"
     ]
    }
   ],
   "source": [
    "# View the transformed 'Interests' column (or other relevant columns)\n",
    "print(content_data[['Gender', 'DOB', 'City', 'Country', 'Interests']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c8be458-f736-4497-a543-5a63e80e459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender         DOB         City    Country Interests\n",
      "0  Female  1958-10-15      Sibolga  Indonesia        []\n",
      "1  Female  2004-07-21     Al Abyār      Libya        []\n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan        []\n",
      "3    Male  1985-04-14       Matera      Italy        []\n",
      "4  Female  1955-09-18      Biruaca  Venezuela        []\n"
     ]
    }
   ],
   "source": [
    "# Remove spaces in 'Interests'\n",
    "content_data['Interests'] = content_data['Interests'].apply(lambda x: [i.replace(\" \", \"\") for i in x])\n",
    "\n",
    "# View the transformed 'Interests' column\n",
    "print(content_data[['Gender', 'DOB', 'City', 'Country', 'Interests']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2d5a5a-7981-41b9-9752-5c7a8fa9c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender         DOB         City    Country Interests\n",
      "0  Female  1958-10-15      Sibolga  Indonesia        []\n",
      "1  Female  2004-07-21     Al Abyār      Libya        []\n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan        []\n",
      "3    Male  1985-04-14       Matera      Italy        []\n",
      "4  Female  1955-09-18      Biruaca  Venezuela        []\n"
     ]
    }
   ],
   "source": [
    "# View the transformed 'Interests' column\n",
    "print(content_data[['Gender', 'DOB', 'City', 'Country', 'Interests']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec0e377-d205-4bb6-88cf-b68c363d6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender         DOB         City    Country                   tags\n",
      "0  Female  1958-10-15      Sibolga  Indonesia   [Sibolga, Indonesia]\n",
      "1  Female  2004-07-21     Al Abyār      Libya      [Al Abyār, Libya]\n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan  [Wādī as Sīr, Jordan]\n",
      "3    Male  1985-04-14       Matera      Italy        [Matera, Italy]\n",
      "4  Female  1955-09-18      Biruaca  Venezuela   [Biruaca, Venezuela]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the 'tags' column by combining 'Interests', 'City', and 'Country'\n",
    "# Ensure all parts are concatenated properly\n",
    "content_data['tags'] = content_data['Interests'] + content_data['City'].apply(lambda x: [x]) + content_data['Country'].apply(lambda x: [x])\n",
    "\n",
    "# View the first few rows of the dataset with the new 'tags' column\n",
    "print(content_data[['Gender', 'DOB', 'City', 'Country', 'tags']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af89259-f93c-4bd8-86f7-e29fcd954f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender         DOB         City    Country                tags\n",
      "0  Female  1958-10-15      Sibolga  Indonesia   Sibolga Indonesia\n",
      "1  Female  2004-07-21     Al Abyār      Libya      Al Abyār Libya\n",
      "2  Female  2000-02-07  Wādī as Sīr     Jordan  Wādī as Sīr Jordan\n",
      "3    Male  1985-04-14       Matera      Italy        Matera Italy\n",
      "4  Female  1955-09-18      Biruaca  Venezuela   Biruaca Venezuela\n"
     ]
    }
   ],
   "source": [
    "# Optional: Join tags into a single string per user (similar to how it was done for movies)\n",
    "content_data['tags'] = content_data['tags'].apply(lambda x: ' '.join(x))  # Join the list into a single string\n",
    "\n",
    "# View the first few rows of the dataset with the new 'tags' column\n",
    "print(content_data[['Gender', 'DOB', 'City', 'Country', 'tags']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3752eb1f-1a90-4086-bf94-9ac6d8b91ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Interests                tags    Country\n",
      "0        []   Sibolga Indonesia  Indonesia\n",
      "1        []      Al Abyār Libya      Libya\n",
      "2        []  Wādī as Sīr Jordan     Jordan\n",
      "3        []        Matera Italy      Italy\n",
      "4        []   Biruaca Venezuela  Venezuela\n"
     ]
    }
   ],
   "source": [
    "# View the first few rows of the dataset with the new 'tags' column\n",
    "print(content_data[['Interests', 'tags','Country']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf08dbc8-2f76-4c33-ad5b-3504569b6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = content_data[['Gender', 'Interests', 'tags','Country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44d5dca7-0ab6-4cf6-9c13-db0910cd0475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Gender', 'DOB', 'Interests', 'City', 'Country', 'tags'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(content_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989e898b-3978-4740-9b03-7d1b3946dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender Interests                tags    Country\n",
      "0  Female        []   Sibolga Indonesia  Indonesia\n",
      "1  Female        []      Al Abyār Libya      Libya\n",
      "2  Female        []  Wādī as Sīr Jordan     Jordan\n",
      "3    Male        []        Matera Italy      Italy\n",
      "4  Female        []   Biruaca Venezuela  Venezuela\n"
     ]
    }
   ],
   "source": [
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1899d86f-e1ac-4155-af00-96c8af6a3520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with max 5000 features and stop words in English\n",
    "cv = CountVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the 'tags' column of new_df to convert text data to numerical data\n",
    "vectorized_data = cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "# View the shape of the resulting matrix\n",
    "print(vectorized_data.shape)\n",
    "\n",
    "# Optional: View the first few rows of the vectorized data\n",
    "print(vectorized_data[5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e18926ab-95ac-40eb-a1bd-dce45c076b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5000)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the 'tags' column of new_df to convert text data into numerical format (vectorized)\n",
    "vectorized_data = cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "# Check the shape of the vectorized data (rows = number of items, columns = features)\n",
    "print(vectorized_data.shape)\n",
    "\n",
    "# Optional: View the first few rows of the vectorized data for inspection\n",
    "print(vectorized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddd54e9c-9262-45a0-b036-43faa45af536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_data[0])\n",
    "print(vectorized_data[1])\n",
    "print(vectorized_data[2])lgorithm='brute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa6d68d4-6947-4bf1-bec7-a7346cb74e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02' '03' '05' ... 'žďár' 'ḩalḩūl' 'ḩawātah']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9089772-128e-48dd-9d22-cb00265d42f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ee534-a294-49c5-b2b4-d7fc9e5975e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da392cf7-2a31-4919-8840-d2ee360bd65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of similar items: [[61883 98725 70758 53374 26772]]\n",
      "Distances from the target item: [[0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Initialize the NearestNeighbors model with cosine similarity\n",
    "nn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "\n",
    "# Fit the model on the vectorized data\n",
    "nn_model.fit(vectorized_data)\n",
    "\n",
    "# Find the top 5 nearest neighbors (or any number you want) for a specific data point (e.g., first user)\n",
    "distances, indices = nn_model.kneighbors([vectorized_data[0]], n_neighbors=5)\n",
    "\n",
    "# Print the indices of the similar users or content and their distances\n",
    "print(\"Indices of similar items:\", indices)\n",
    "print(\"Distances from the target item:\", distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ba289a7-8e52-4ac5-b1c3-b2200f44304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar items: [214 197  91  90   0]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "# Assume the dataset is already loaded and vectorized\n",
    "# Create vectorized data (assuming 'tags' column is processed already)\n",
    "cv = CountVectorizer(max_features=5000, stop_words='english')\n",
    "vectorized_data = cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "# Normalize the vectorized data to have unit norm (L2 norm)\n",
    "vectorized_data = vectorized_data.astype('float32')\n",
    "faiss.normalize_L2(vectorized_data)\n",
    "\n",
    "# Initialize a FAISS index for inner product search\n",
    "dimension = vectorized_data.shape[1]  # Number of features (5000 in this case)\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product search\n",
    "\n",
    "# Add vectorized data to the FAISS index\n",
    "index.add(vectorized_data)\n",
    "\n",
    "# Query the FAISS index to find the 5 most similar items (including itself)\n",
    "k = 6  # 1 for the item itself + 5 similar items\n",
    "query_vector = vectorized_data[0].reshape(1, -1)  # Example: querying for the first item\n",
    "distances, indices = index.search(query_vector, k)  # Perform the search\n",
    "\n",
    "# Print the indices of the 5 most similar items (excluding the item itself)\n",
    "print(\"Top 5 similar items:\", indices[0][1:])  # Exclude the first result (itself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3d5fd6c-1de8-41ea-b751-c92b62db65bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Interests Data:\n",
      "0              'Movies', 'Fashion', 'Fashion', 'Books'\n",
      "1    'Gaming', 'Finance and investments', 'Outdoor ...\n",
      "2      'DIY and crafts', 'Music', 'Science', 'Fashion'\n",
      "3         'Outdoor activities', 'Cars and automobiles'\n",
      "4                                'Politics', 'History'\n",
      "Name: Interests, dtype: object\n",
      "Processed Interests Data:\n",
      "0    []\n",
      "1    []\n",
      "2    []\n",
      "3    []\n",
      "4    []\n",
      "Name: Interests, dtype: object\n",
      "Tags Data:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "4    \n",
      "Name: tags, dtype: object\n",
      "Distances: [[1. 1. 1. 1. 1. 1.]]\n",
      "Indices: [[279 214 197  91  90   0]]\n",
      "Recommended Items: [list([]) list([]) list([]) list([]) list([])]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "content_data = pd.read_csv('SocialMediaUsersDataset.csv')\n",
    "\n",
    "# Check the structure of the Interests column\n",
    "print(\"Initial Interests Data:\")\n",
    "print(content_data['Interests'].head())\n",
    "\n",
    "# Define a function to convert string representation of lists into actual lists\n",
    "def convert(obj):\n",
    "    L = []\n",
    "    if isinstance(obj, str):  # Ensure obj is a string\n",
    "        try:\n",
    "            # Safely evaluate the string representation of the list of dictionaries\n",
    "            items = ast.literal_eval(obj)\n",
    "            if isinstance(items, list):  # Check if items is a list\n",
    "                for i in items:\n",
    "                    if isinstance(i, dict) and 'name' in i:  # Ensure each item is a dict with 'name'\n",
    "                        L.append(i['name'])\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []  # Return an empty list if there is any issue\n",
    "    return L\n",
    "\n",
    "# Apply the convert function to the relevant columns\n",
    "content_data['Interests'] = content_data['Interests'].apply(convert)\n",
    "\n",
    "# Create a new tags column by combining relevant information\n",
    "content_data['tags'] = content_data['Interests'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Print the processed data\n",
    "print(\"Processed Interests Data:\")\n",
    "print(content_data['Interests'].head())\n",
    "print(\"Tags Data:\")\n",
    "print(content_data['tags'].head())\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, stop_words='english')\n",
    "#vectorized_data = cv.fit_transform(content_data['tags']).toarray()\n",
    "\n",
    "# Create a Faiss index\n",
    "dimension = vectorized_data.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Use Inner Product\n",
    "index.add(np.array(vectorized_data).astype('float32'))  # Add the vectors to the index\n",
    "\n",
    "# Recommendation function\n",
    "def recommend(item_index, num_recommendations=5):\n",
    "    # Get the vector of the item we want to find recommendations for\n",
    "    item_vector = vectorized_data[item_index].reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search for the nearest neighbors in the index\n",
    "    distances, indices = index.search(item_vector, num_recommendations + 1)  # +1 to exclude the item itself\n",
    "\n",
    "    # Print distances and indices for debugging\n",
    "    print(\"Distances:\", distances)\n",
    "    print(\"Indices:\", indices)\n",
    "\n",
    "    # Get the recommended item indices (excluding the first one which is the item itself)\n",
    "    recommended_indices = indices[0][1:]\n",
    "\n",
    "    # Get the names (or titles) of the recommended items\n",
    "    recommended_items = content_data.iloc[recommended_indices]['Interests'].values\n",
    "\n",
    "    return recommended_items\n",
    "\n",
    "# Example usage\n",
    "item_index = 0  # Change this to the desired item's index for recommendations\n",
    "recommended_items = recommend(item_index=item_index, num_recommendations=5)  # Change num_recommendations as needed\n",
    "print(\"Recommended Items:\", recommended_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c51cbaa-cff1-4377-98c9-a5e4b16ac559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as 'social_media_recommendations.pkl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "content_data = pd.read_csv('SocialMediaUsersDataset.csv')\n",
    "\n",
    "# Assuming you've processed content_data as per your earlier code\n",
    "# Here is a quick outline of how it might look\n",
    "# Define a function to convert string representation of lists into actual lists\n",
    "def convert(obj):\n",
    "    L = []\n",
    "    if isinstance(obj, str):  # Ensure obj is a string\n",
    "        try:\n",
    "            items = ast.literal_eval(obj)\n",
    "            if isinstance(items, list):  # Check if items is a list\n",
    "                for i in items:\n",
    "                    if isinstance(i, dict) and 'name' in i:\n",
    "                        L.append(i['name'])\n",
    "        except (ValueError, SyntaxError):\n",
    "            return []  # Return an empty list if there is any issue\n",
    "    return L\n",
    "\n",
    "# Apply the convert function to the relevant columns\n",
    "content_data['Interests'] = content_data['Interests'].apply(convert)\n",
    "\n",
    "# Create a new tags column by combining relevant information\n",
    "content_data['tags'] = content_data['Interests'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the processed DataFrame `content_data` as a pickle file\n",
    "with open('social_media_recommendations.pkl', 'wb') as f:\n",
    "    pickle.dump(content_data, f)\n",
    "\n",
    "print(\"DataFrame saved as 'social_media_recommendations.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9b63029-242f-41da-bd88-dcafffbc634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interests:\n",
      "[list([]) list([]) list([]) ... list([]) list([]) list([])]\n",
      "First 5 Interests:\n",
      "[list([]) list([]) list([]) list([]) list([])]\n"
     ]
    }
   ],
   "source": [
    "# Assuming content_data is already defined and has an 'Interests' column\n",
    "interests = content_data['Interests'].values\n",
    "print(\"Interests:\")\n",
    "print(interests)\n",
    "\n",
    "# To check the first few interests\n",
    "print(\"First 5 Interests:\")\n",
    "print(interests[:5])  # Prints the first 5 interests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15fcc6f6-d46e-4f60-850a-e03a7926d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the DataFrame `content_data` as a dictionary in a pickle file\n",
    "pickle.dump(content_data.to_dict(), open('social_media_dict.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22a6af53-3fd0-44c3-b7f5-2fbf3941d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items: [list([]) list([]) list([]) list([]) list([])]\n"
     ]
    }
   ],
   "source": [
    "# Recommendation function\n",
    "def recommend(item_index, num_recommendations=5):\n",
    "    # Get the vector of the item we want to find recommendations for\n",
    "    item_vector = vectorized_data[item_index].reshape(1, -1).astype('float32')\n",
    "\n",
    "    # Search for the nearest neighbors in the index\n",
    "    distances, indices = index.search(item_vector, num_recommendations + 1)  # +1 to exclude the item itself\n",
    "\n",
    "    # Get the recommended item indices (excluding the first one which is the item itself)\n",
    "    recommended_indices = indices[0][1:]\n",
    "\n",
    "    # Get the names (or titles) of the recommended items\n",
    "    recommended_items = content_data.iloc[recommended_indices]['Interests'].values  # Adjust if necessary\n",
    "\n",
    "    return recommended_items\n",
    "\n",
    "# Example usage\n",
    "recommended_items = recommend(item_index=0, num_recommendations=5)  # Change item_index to the desired item's index\n",
    "print(\"Recommended Items:\", recommended_items)\n",
    "\n",
    "# Save the content data DataFrame\n",
    "pickle.dump(content_data.to_dict(), open('content_data_dict.pkl', 'wb'))\n",
    "\n",
    "# Save the vectorized data\n",
    "pickle.dump(vectorized_data, open('vectorized_data.pkl', 'wb'))\n",
    "\n",
    "# Save the Faiss index\n",
    "faiss.write_index(index, 'faiss_index.index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b43142-f8b6-4935-9637-5ceed26ada73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84facd-8c84-419e-96bd-6febc0c17feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3cc57-c4d7-4e8a-8f13-09c757acf84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c0213-0444-4a53-bdd8-2ba5ab450e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58435e7-685c-492f-9f4d-56ed4b248e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034c532-89e6-465b-8f3d-1a7c5a52bc40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
